{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train data  (182080, 16)\n",
      "Shape of Train data  (1541272, 4)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The attributes in Resources_data are as folows    ->   ->   ->   ->   ->  ['id' 'description' 'quantity' 'price']\n",
      "The attributes in Project_data are as folows    ->    ->    ->    ->    ->   ['id' 'teacher_id' 'teacher_prefix' 'school_state'\n",
      " 'project_submitted_datetime' 'project_grade_category'\n",
      " 'project_subject_categories' 'project_subject_subcategories'\n",
      " 'project_title' 'project_essay_1' 'project_essay_2' 'project_essay_3'\n",
      " 'project_essay_4' 'project_resource_summary'\n",
      " 'teacher_number_of_previously_posted_projects' 'project_is_approved']\n"
     ]
    }
   ],
   "source": [
    "Project_data = pd.read_csv(\"train.csv\")\n",
    "Resources_data = pd.read_csv(\"resources.csv\")\n",
    "print(\"Shape of Train data \",Project_data.shape)\n",
    "print(\"Shape of Train data \",Resources_data.shape)\n",
    "print(\"-\"*100)\n",
    "print(\"The attributes in Resources_data are as folows \" , \"  -> \"*5,Resources_data.columns.values)\n",
    "print(\"The attributes in Project_data are as folows \" , \"  ->  \"*5,Project_data.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128730</th>\n",
       "      <td>p146022</td>\n",
       "      <td>f91a89d2b72a0f5ee14cd98bd8741fc2</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>CA</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>Math &amp; Science</td>\n",
       "      <td>Applied Sciences</td>\n",
       "      <td>Robotics and Programming</td>\n",
       "      <td>I love giving my students experiences. A new e...</td>\n",
       "      <td>My students can vary quite dramatically. I hav...</td>\n",
       "      <td>The student will use the Sparki Robots to lear...</td>\n",
       "      <td>The Sparki robots would add a motivation for t...</td>\n",
       "      <td>My students need four Sparki robots to help st...</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-27 00:03:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173035</th>\n",
       "      <td>p244738</td>\n",
       "      <td>64ef8a335f7206366c52c39f5bfd09b7</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>TX</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Literacy &amp; Language, Math &amp; Science</td>\n",
       "      <td>Foreign Languages, Mathematics</td>\n",
       "      <td>Help Us Finish Our 1st Year In School!</td>\n",
       "      <td>We are getting closer to the end of the year, ...</td>\n",
       "      <td>Welcome to our Pre-K classroom. We work hard e...</td>\n",
       "      <td>This time, I am asking to improve three differ...</td>\n",
       "      <td>This project will have a huge impact on my stu...</td>\n",
       "      <td>My students need Linking Letter Monkeys and Sp...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-27 00:04:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                        teacher_id teacher_prefix school_state  \\\n",
       "128730  p146022  f91a89d2b72a0f5ee14cd98bd8741fc2            Ms.           CA   \n",
       "173035  p244738  64ef8a335f7206366c52c39f5bfd09b7            Ms.           TX   \n",
       "\n",
       "       project_grade_category           project_subject_categories  \\\n",
       "128730             Grades 6-8                       Math & Science   \n",
       "173035          Grades PreK-2  Literacy & Language, Math & Science   \n",
       "\n",
       "         project_subject_subcategories  \\\n",
       "128730                Applied Sciences   \n",
       "173035  Foreign Languages, Mathematics   \n",
       "\n",
       "                                 project_title  \\\n",
       "128730                Robotics and Programming   \n",
       "173035  Help Us Finish Our 1st Year In School!   \n",
       "\n",
       "                                          project_essay_1  \\\n",
       "128730  I love giving my students experiences. A new e...   \n",
       "173035  We are getting closer to the end of the year, ...   \n",
       "\n",
       "                                          project_essay_2  \\\n",
       "128730  My students can vary quite dramatically. I hav...   \n",
       "173035  Welcome to our Pre-K classroom. We work hard e...   \n",
       "\n",
       "                                          project_essay_3  \\\n",
       "128730  The student will use the Sparki Robots to lear...   \n",
       "173035  This time, I am asking to improve three differ...   \n",
       "\n",
       "                                          project_essay_4  \\\n",
       "128730  The Sparki robots would add a motivation for t...   \n",
       "173035  This project will have a huge impact on my stu...   \n",
       "\n",
       "                                 project_resource_summary  \\\n",
       "128730  My students need four Sparki robots to help st...   \n",
       "173035  My students need Linking Letter Monkeys and Sp...   \n",
       "\n",
       "        teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "128730                                            30                    1   \n",
       "173035                                             1                    1   \n",
       "\n",
       "                      Date  \n",
       "128730 2016-04-27 00:03:38  \n",
       "173035 2016-04-27 00:04:09  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here the code is chainging the date and time format used in the data to a standard form  and replacing the data with the new format and \n",
    "# replacing the same coloum  refrence to same are given below\n",
    "\n",
    "Project_data[\"Date\"] = pd.to_datetime(Project_data[\"project_submitted_datetime\"])\n",
    "Project_data.drop(\"project_submitted_datetime\",axis=1,inplace = True )\n",
    "Project_data.sort_values(by=[\"Date\"],inplace=True)\n",
    "Project_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of NAN values in teacher_prefix Column is 0\n"
     ]
    }
   ],
   "source": [
    "#find out the NAN values in the dataframe and fill with null\n",
    "\n",
    "Project_data[\"teacher_prefix\"] = Project_data[\"teacher_prefix\"].fillna(\"null\")\n",
    "print(\"The number of NAN values in teacher_prefix Column is \" + str(Project_data[\"teacher_prefix\"].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Worlds in sorted_cat_dict {'warmth': 2327, 'care_hunger': 2327, 'history_civics': 9726, 'music_arts': 17054, 'appliedlearning': 20303, 'specialneeds': 22881, 'health_sports': 23712, 'math_science': 69248, 'literacy_language': 86988}\n"
     ]
    }
   ],
   "source": [
    "# remove special characters from list of strings python: https://stackoverflow.com/a/47301924/4084039\n",
    "\n",
    "catogories = list(Project_data['project_subject_categories'].values)\n",
    "cat_list = []\n",
    "for i in catogories:\n",
    "    temp = \"\"\n",
    "    for j in i.split(','):\n",
    "        if 'The' in j.split():\n",
    "            j=j.replace('The','')\n",
    "        j = j.replace(' ','')\n",
    "        temp+=j.strip()+\" \"\n",
    "        temp = temp.replace('&','_') \n",
    "    cat_list.append(temp.strip().lower())\n",
    "    \n",
    "Project_data['clean_categories'] = cat_list\n",
    "Project_data.drop(['project_subject_categories'], axis=1, inplace=True)\n",
    "\n",
    "from collections import Counter\n",
    "my_counter = Counter()\n",
    "for word in Project_data['clean_categories'].values:\n",
    "    my_counter.update(word.split())\n",
    "\n",
    "cat_dict = dict(my_counter)\n",
    "sorted_cat_dict = dict(sorted(cat_dict.items(), key=lambda kv: kv[1]))\n",
    "print(\"The Worlds in sorted_cat_dict\",sorted_cat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Worlds in sorted_cat_dict1 {'economics': 431, 'communityservice': 712, 'financialliteracy': 956, 'parentinvolvement': 1103, 'civics_government': 1294, 'extracurricular': 1332, 'foreignlanguages': 1496, 'nutritioneducation': 2236, 'warmth': 2327, 'care_hunger': 2327, 'socialsciences': 3149, 'performingarts': 3223, 'charactereducation': 3519, 'teamsports': 3643, 'other': 3948, 'college_careerprep': 4291, 'music': 5172, 'history_geography': 5258, 'earlydevelopment': 7149, 'esl': 7162, 'health_lifescience': 7175, 'gym_fitness': 7592, 'environmentalscience': 9436, 'visualarts': 10399, 'health_wellness': 17099, 'appliedsciences': 17966, 'specialneeds': 22881, 'literature_writing': 36974, 'mathematics': 46884, 'literacy': 56066}\n",
      "economics 431\n",
      "communityservice 712\n",
      "financialliteracy 956\n",
      "parentinvolvement 1103\n",
      "civics_government 1294\n",
      "extracurricular 1332\n",
      "foreignlanguages 1496\n",
      "nutritioneducation 2236\n",
      "warmth 2327\n",
      "care_hunger 2327\n",
      "socialsciences 3149\n",
      "performingarts 3223\n",
      "charactereducation 3519\n",
      "teamsports 3643\n",
      "other 3948\n",
      "college_careerprep 4291\n",
      "music 5172\n",
      "history_geography 5258\n",
      "earlydevelopment 7149\n",
      "esl 7162\n",
      "health_lifescience 7175\n",
      "gym_fitness 7592\n",
      "environmentalscience 9436\n",
      "visualarts 10399\n",
      "health_wellness 17099\n",
      "appliedsciences 17966\n",
      "specialneeds 22881\n",
      "literature_writing 36974\n",
      "mathematics 46884\n",
      "literacy 56066\n"
     ]
    }
   ],
   "source": [
    "catogories1 = list(Project_data['project_subject_subcategories'].values)\n",
    "cat_list1 = []\n",
    "for i in catogories1:\n",
    "    temp1 = \"\"\n",
    "    for j in i.split(','): \n",
    "        if 'The' in j.split():\n",
    "            j=j.replace('The','')\n",
    "        j = j.replace(' ','') \n",
    "        temp1+=j.strip()+\" \" \n",
    "        temp1 = temp1.replace('&','_') \n",
    "    cat_list1.append(temp1.strip().lower())\n",
    "    \n",
    "Project_data['clean_sub_categories'] = cat_list1\n",
    "Project_data.drop(['project_subject_subcategories'], axis=1, inplace=True)\n",
    "\n",
    "from collections import Counter\n",
    "my_counter1 = Counter()\n",
    "for word in Project_data['clean_sub_categories'].values:\n",
    "    my_counter1.update(word.split())\n",
    "\n",
    "cat_dict1 = dict(my_counter1)\n",
    "sorted_cat_dict1 = dict(sorted(cat_dict1.items(), key=lambda kv: kv[1]))\n",
    "print(\"The Worlds in sorted_cat_dict1\",sorted_cat_dict1)\n",
    "for i in sorted_cat_dict1:\n",
    "    print(i,sorted_cat_dict1[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Values in sorted_school_state_dict :  {'vt': 139, 'wy': 177, 'nd': 248, 'mt': 384, 'ri': 475, 'ne': 493, 'sd': 494, 'ak': 557, 'nh': 587, 'de': 589, 'me': 827, 'wv': 839, 'hi': 869, 'dc': 902, 'nm': 916, 'ks': 1060, 'ia': 1079, 'id': 1113, 'ar': 1757, 'co': 1887, 'mn': 2055, 'or': 2119, 'ky': 2172, 'ms': 2222, 'nv': 2297, 'md': 2480, 'ct': 2766, 'ut': 2814, 'tn': 2836, 'al': 2955, 'wi': 2983, 'va': 3446, 'az': 3614, 'nj': 3671, 'ok': 3829, 'wa': 3903, 'la': 3946, 'ma': 4054, 'oh': 4139, 'mo': 4247, 'in': 4314, 'pa': 5093, 'mi': 5324, 'sc': 6463, 'ga': 6636, 'il': 7332, 'nc': 8463, 'fl': 10359, 'ny': 12157, 'tx': 12304, 'ca': 25695}\n"
     ]
    }
   ],
   "source": [
    "school_state = list(Project_data['school_state'].values)\n",
    "school_state_list = []\n",
    "for i in school_state:\n",
    "    temp2 = \"\"\n",
    "    for j in i.split(','):\n",
    "        if 'The' in j.split():\n",
    "            j=j.replace('The','') \n",
    "        j = j.replace(' ','') \n",
    "        temp2 +=j.strip()+\" \"\n",
    "        temp2 = temp2.replace('&','_')\n",
    "    school_state_list.append(temp2.strip().lower())\n",
    "\n",
    "Project_data['School_state'] = school_state_list\n",
    "Project_data.drop(['school_state'], axis=1, inplace=True)\n",
    "\n",
    "my_counter3 = Counter()\n",
    "for word in Project_data['School_state'].values:\n",
    "    my_counter3.update(word.split())\n",
    "    \n",
    "school_state_dict = dict(my_counter3)\n",
    "sorted_school_state_dict = dict(sorted(school_state_dict.items(), key=lambda kv: kv[1]))\n",
    "print(\"The Values in sorted_school_state_dict : \", sorted_school_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two column text dataframe: \n",
    "\n",
    "\n",
    "Project_data[\"essay\"] =Project_data[\"project_essay_1\"].map(str) +\\\n",
    "                        Project_data[\"project_essay_2\"].map(str) +\\\n",
    "                        Project_data[\"project_essay_3\"].map(str) +\\\n",
    "                        Project_data[\"project_essay_4\"].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    \n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    \n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "\n",
    "\n",
    "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 182080/182080 [02:33<00:00, 1183.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Combining all the above stundents \n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "preprocessed_essays = []\n",
    "\n",
    "for sentance in tqdm(Project_data['essay'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    \n",
    "    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "    preprocessed_essays.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 182080/182080 [00:06<00:00, 26058.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "preprocessed_titles = []\n",
    "\n",
    "for sentence in tqdm(Project_data['project_title'].values):\n",
    "    sent = decontracted(sentence)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    \n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "    preprocessed_titles.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project_data[\"clean_titles\"] = preprocessed_titles\n",
    "Project_data.drop([\"project_essay_1\"],axis=1,inplace=True)\n",
    "Project_data.drop([\"project_essay_2\"],axis=1,inplace=True)\n",
    "Project_data.drop([\"project_essay_3\"],axis=1,inplace=True)\n",
    "Project_data.drop([\"project_essay_4\"],axis=1,inplace=True)\n",
    "Project_data.drop([\"project_title\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project_data Columns are - >>>  Index(['id', 'teacher_id', 'teacher_prefix', 'project_grade_category',\n",
      "       'project_resource_summary',\n",
      "       'teacher_number_of_previously_posted_projects', 'project_is_approved',\n",
      "       'Date', 'clean_categories', 'clean_sub_categories', 'School_state',\n",
      "       'essay', 'clean_titles'],\n",
      "      dtype='object') Resources_data Columns   Index(['id', 'description', 'quantity', 'price'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Project_data Columns are - >>> \" , Project_data.columns ,\"Resources_data Columns  \", Resources_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p000001</td>\n",
       "      <td>459.56</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p000002</td>\n",
       "      <td>515.89</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   price  quantity\n",
       "0  p000001  459.56         7\n",
       "1  p000002  515.89        21"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Price_data = Resources_data.groupby(\"id\").agg({\"price\": \"sum\" ,\"quantity\" : \"sum\"}).reset_index()\n",
    "Price_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of NAN values in PRoject_data :  id                                              0\n",
      "teacher_id                                      0\n",
      "teacher_prefix                                  0\n",
      "project_grade_category                          0\n",
      "project_resource_summary                        0\n",
      "teacher_number_of_previously_posted_projects    0\n",
      "project_is_approved                             0\n",
      "Date                                            0\n",
      "clean_categories                                0\n",
      "clean_sub_categories                            0\n",
      "School_state                                    0\n",
      "essay                                           0\n",
      "clean_titles                                    0\n",
      "price                                           0\n",
      "quantity                                        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Project_data = pd.merge(Project_data,Price_data,on=\"id\",how = \"left\")\n",
    "print(\"The Number of NAN values in PRoject_data : \",Project_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'teacher_id', 'teacher_prefix', 'project_grade_category',\n",
      "       'project_resource_summary',\n",
      "       'teacher_number_of_previously_posted_projects', 'Date',\n",
      "       'clean_categories', 'clean_sub_categories', 'School_state', 'essay',\n",
      "       'clean_titles', 'price', 'quantity'],\n",
      "      dtype='object')\n",
      "(182080, 15)\n"
     ]
    }
   ],
   "source": [
    "# Project_data = Project_data.head(50000)\n",
    "\n",
    "Y = Project_data[\"project_is_approved\"].values\n",
    "X = Project_data.drop([\"project_is_approved\"],axis = 1)\n",
    "print(X.columns)\n",
    "print(Project_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into train and test data_set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, stratify= Project_data['project_is_approved'],random_state=0)\n",
    "X_train, X_cv, Y_train, Y_cv = train_test_split(X_train, Y_train, test_size=0.33, stratify=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({0: 15227, 1: 15227})\n",
      "Capitial X represents the original train_data and lower case x represnts the ramdonly over-sampled data\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ros = RandomOverSampler(sampling_strategy='minority',random_state=42)\n",
    "x_train, y_train = ros.fit_resample(X_train, Y_train)\n",
    "print('Resampled dataset shape %s' % Counter(y_train))\n",
    "print(\"Capitial\" ,\"X\",\"represents the original train_data and lower case\" ,\"x\", \"represnts the ramdonly over-sampled data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30454, 14)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = pd.DataFrame(x_train,columns = X.columns)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train data  (30454, 14) (30454,)\n",
      "====================================================================================================\n",
      "Shape of Train CV  data  (10490, 14) (10490,)\n",
      "====================================================================================================\n",
      "Shape of Test data  (787, 14) (787,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Train data \",x_train.shape, y_train.shape)\n",
    "print(\"=\"*100)\n",
    "print(\"Shape of Train CV  data \",X_cv.shape, Y_cv.shape)\n",
    "print(\"=\"*100)\n",
    "print(\"Shape of Test data \",X_test.shape, Y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations of the clean_sub_categories , One-hot-encoding shape of the data is\n",
      "Shape of Train data  (30454, 30) (30454,)\n",
      "Shape of Train CV  data  (10490, 30) (10490,)\n",
      "Shape of Test data  (787, 30) (787,)\n"
     ]
    }
   ],
   "source": [
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "\n",
    "vectorizer1 = CountVectorizer()\n",
    "vectorizer1.fit(x_train['clean_sub_categories']) \n",
    "\n",
    "x_train_clean_subcat_ohe = vectorizer1.transform(x_train['clean_sub_categories'])\n",
    "X_cv_clean_subcat_ohe = vectorizer1.transform(X_cv['clean_sub_categories'])\n",
    "X_test_clean_subcat_ohe = vectorizer1.transform(X_test['clean_sub_categories'])\n",
    "\n",
    "print(\"After vectorizations of the clean_sub_categories , One-hot-encoding shape of the data is\")\n",
    "print(\"Shape of Train data \",x_train_clean_subcat_ohe.shape, y_train.shape)\n",
    "print(\"Shape of Train CV  data \",X_cv_clean_subcat_ohe.shape, Y_cv.shape)\n",
    "print(\"Shape of Test data \" ,X_test_clean_subcat_ohe.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations of the clean_categories, One-hot-encoding shape of the data is\n",
      "Shape of Train data  (30454, 9) (30454,)\n",
      "Shape of Train CV  data  (10490, 9) (10490,)\n",
      "Shape of Test data  (787, 9) (787,)\n"
     ]
    }
   ],
   "source": [
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "\n",
    "vectorizer2 = CountVectorizer()\n",
    "vectorizer2.fit(x_train['clean_categories']) \n",
    "\n",
    "\n",
    "\n",
    "x_train_clean_categories_ohe = vectorizer2.transform(x_train['clean_categories'])\n",
    "X_cv_clean_categories_ohe = vectorizer2.transform(X_cv['clean_categories'])\n",
    "X_test_clean_categories_ohe = vectorizer2.transform(X_test['clean_categories'])\n",
    "\n",
    "print(\"After vectorizations of the clean_categories, One-hot-encoding shape of the data is\")\n",
    "print(\"Shape of Train data \",x_train_clean_categories_ohe.shape, y_train.shape)\n",
    "print(\"Shape of Train CV  data \",X_cv_clean_categories_ohe.shape, Y_cv.shape)\n",
    "print(\"Shape of Test data \" ,X_test_clean_categories_ohe.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations of the teacher_prefix , One-hot-encoding shape of the data is\n",
      "Shape of Train data  (30454, 5) (30454,)\n",
      "Shape of Train CV  data  (10490, 5) (10490,)\n",
      "Shape of Test data  (787, 5) (787,)\n"
     ]
    }
   ],
   "source": [
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "\n",
    "vectorizer3 = CountVectorizer()\n",
    "vectorizer3.fit(x_train['teacher_prefix'].values.astype('U')) \n",
    "\n",
    "\n",
    "x_train_teacher_ohe = vectorizer3.transform(x_train['teacher_prefix'].values.astype('U'))\n",
    "X_cv_teacher_ohe = vectorizer3.transform(X_cv['teacher_prefix'].values.astype('U'))\n",
    "X_test_teacher_ohe = vectorizer3.transform(X_test['teacher_prefix'].values.astype('U'))\n",
    "\n",
    "print(\"After vectorizations of the teacher_prefix , One-hot-encoding shape of the data is\")\n",
    "print(\"Shape of Train data \",x_train_teacher_ohe.shape, y_train.shape)\n",
    "print(\"Shape of Train CV  data \",X_cv_teacher_ohe.shape, Y_cv.shape)\n",
    "print(\"Shape of Test data \" ,X_test_teacher_ohe.shape, Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations of the School_state , One-hot-encoding shape of the data is\n",
      "Shape of Train data  (30454, 51) (30454,)\n",
      "Shape of Train CV  data  (10490, 51) (10490,)\n",
      "Shape of Test data  (787, 51) (787,)\n"
     ]
    }
   ],
   "source": [
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "\n",
    "vectorizer4 = CountVectorizer()\n",
    "vectorizer4.fit(x_train['School_state'].values) \n",
    "\n",
    "\n",
    "x_train_state_ohe = vectorizer4.transform(x_train['School_state'].values)\n",
    "X_cv_state_ohe = vectorizer4.transform(X_cv['School_state'].values)\n",
    "X_test_state_ohe = vectorizer4.transform(X_test['School_state'].values)\n",
    "\n",
    "print(\"After vectorizations of the School_state , One-hot-encoding shape of the data is\")\n",
    "print(\"Shape of Train data \",x_train_state_ohe.shape, y_train.shape)\n",
    "print(\"Shape of Train CV  data \",X_cv_state_ohe.shape, Y_cv.shape)\n",
    "print(\"Shape of Test data \" ,X_test_state_ohe.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'grades_9_12': 2240, 'grades_6_8': 3422, 'grades_3_5': 6968, 'grades_prek_2': 8666}\n"
     ]
    }
   ],
   "source": [
    "grade_cat_list = []\n",
    "for grade in X_train['project_grade_category'].values:\n",
    "    grade = grade.replace(\"-\",\"_\").lower()\n",
    "    grade = grade.replace(\" \",\"_\").lower()\n",
    "    grade_cat_list.append(grade)\n",
    "\n",
    "\n",
    "X_train['clean_grade'] = grade_cat_list\n",
    "X_train.drop(['project_grade_category'], axis=1, inplace=True)\n",
    "\n",
    "my_counter = Counter()\n",
    "for word in X_train['clean_grade'].values:\n",
    "     my_counter.update(word.split())\n",
    "project_grade_category_dict= dict(my_counter)\n",
    "sorted_project_grade_category_dict = dict(sorted(project_grade_category_dict.items(), key=lambda kv: kv[1]))\n",
    "print(sorted_project_grade_category_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations of the project_grade_category , One-hot-encoding shape of the data is\n",
      "Shape of Train data  (30454, 4) (30454,)\n",
      "Shape of Train CV  data  (10490, 4) (10490,)\n",
      "Shape of Test data  (787, 4) (787,)\n"
     ]
    }
   ],
   "source": [
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "\n",
    "vectorizer5 = CountVectorizer(vocabulary=list(sorted_project_grade_category_dict.keys()), lowercase=False, binary=True)\n",
    "vectorizer5.fit(x_train['project_grade_category'].values) \n",
    "\n",
    "x_train_grade_ohe = vectorizer5.transform(x_train['project_grade_category'].values)\n",
    "X_cv_grade_ohe = vectorizer5.transform(X_cv['project_grade_category'].values)\n",
    "X_test_grade_ohe = vectorizer5.transform(X_test['project_grade_category'].values)\n",
    "\n",
    "print(\"After vectorizations of the project_grade_category , One-hot-encoding shape of the data is\")\n",
    "print(\"Shape of Train data \",x_train_grade_ohe.shape, y_train.shape)\n",
    "print(\"Shape of Train CV  data \",X_cv_grade_ohe.shape, Y_cv.shape)\n",
    "print(\"Shape of Test data \" ,X_test_grade_ohe.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations of the price data ,  shape of the data after standazing\n",
      "(30454, 1) (30454,)\n",
      "(10490, 1) (10490,)\n",
      "(787, 1) (787,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "standard_vector1 = Normalizer()\n",
    "\n",
    "standard_vector1.fit(x_train['price'].values.reshape(-1,1))\n",
    "\n",
    "x_train_price_std = standard_vector1.transform(x_train['price'].values.reshape(-1,1))\n",
    "X_cv_price_std = standard_vector1.transform(X_cv['price'].values.reshape(-1,1))\n",
    "X_test_price_std = standard_vector1.transform(X_test['price'].values.reshape(-1,1))\n",
    "\n",
    "print(\"After vectorizations of the price data ,  shape of the data after standazing\")\n",
    "print(x_train_price_std.shape, y_train.shape)\n",
    "print(X_cv_price_std.shape, Y_cv.shape)\n",
    "print(X_test_price_std.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations of the teacher_number_of_previously_posted_projects , shape of the data after standazing\n",
      "(30454, 1) (30454,)\n",
      "(10490, 1) (10490,)\n",
      "(787, 1) (787,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "standard_vector2 = Normalizer()\n",
    "\n",
    "standard_vector2.fit(x_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\n",
    "\n",
    "x_train_projects_std = standard_vector2.transform(x_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\n",
    "X_cv_projects_std = standard_vector2.transform(X_cv['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\n",
    "X_test_projects_std = standard_vector2.transform(X_test['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\n",
    "\n",
    "print(\"After vectorizations of the teacher_number_of_previously_posted_projects , shape of the data after standazing\")\n",
    "print(x_train_projects_std.shape, y_train.shape)\n",
    "print(X_cv_projects_std.shape,Y_cv.shape)\n",
    "print(X_test_projects_std.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(30454, 1) (30454,)\n",
      "(10490, 1) (10490,)\n",
      "(787, 1) (787,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "standard_vector3 = Normalizer()\n",
    "\n",
    "standard_vector3.fit(x_train['quantity'].values.reshape(-1,1))\n",
    "\n",
    "x_train_qty_std = standard_vector3.transform(x_train['quantity'].values.reshape(-1,1))\n",
    "X_cv_qty_std = standard_vector3.transform(X_cv['quantity'].values.reshape(-1,1))\n",
    "X_test_qty_std = standard_vector3.transform(X_test['quantity'].values.reshape(-1,1))\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(x_train_qty_std.shape, y_train.shape)\n",
    "print(X_cv_qty_std.shape, Y_cv.shape)\n",
    "print(X_test_qty_std.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations of the clean_titles(Project Titles) data the shape of the data is\n",
      "Shape of the x_train data after Vectorization using BOW  (30454, 2644) (30454,)\n",
      "Shape of the X_cv data after Vectorization using BOW  (10490, 2644) (10490,)\n",
      "Shape of the X_test_ data after Vectorization using BOW  (787, 2644) (787,)\n"
     ]
    }
   ],
   "source": [
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer7 = CountVectorizer(min_df=5,tokenizer = lambda x: x.split(), max_features=5000)\n",
    "vectorizer7.fit(x_train['clean_titles'].values) \n",
    "\n",
    "\n",
    "x_train_titles_bow = vectorizer7.transform(x_train['clean_titles'].values)\n",
    "X_cv_titles_bow = vectorizer7.transform(X_cv['clean_titles'].values)\n",
    "X_test_titles_bow = vectorizer7.transform(X_test['clean_titles'].values)\n",
    "\n",
    "print(\"After vectorizations of the clean_titles(Project Titles) data the shape of the data is\")\n",
    "print(\"Shape of the x_train data after Vectorization using BOW \" ,x_train_titles_bow.shape, y_train.shape)\n",
    "print(\"Shape of the X_cv data after Vectorization using BOW \" ,X_cv_titles_bow.shape, Y_cv.shape)\n",
    "print(\"Shape of the X_test_ data after Vectorization using BOW \" ,X_test_titles_bow.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations of the project_resource_summary data the shape of the data is\n",
      "Shape of the x_train data after Vectorization using BOW  (30454, 10000) (30454,)\n",
      "Shape of the X_cv data after Vectorization using BOW  (10490, 10000) (10490,)\n",
      "Shape of the X_test_ data after Vectorization using BOW  (787, 10000) (787,)\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer8 = CountVectorizer(min_df=5,tokenizer = lambda x: x.split(), max_features=10000,ngram_range=(1, 4))\n",
    "vectorizer8.fit(x_train['project_resource_summary']) \n",
    "\n",
    "\n",
    "x_train_summary_bow = vectorizer8.transform(x_train['project_resource_summary'])\n",
    "X_cv_summary_bow = vectorizer8.transform(X_cv['project_resource_summary'])\n",
    "X_test_summary_bow = vectorizer8.transform(X_test['project_resource_summary'])\n",
    "\n",
    "print(\"After vectorizations of the project_resource_summary data the shape of the data is\")\n",
    "print(\"Shape of the x_train data after Vectorization using BOW \" ,x_train_summary_bow.shape, y_train.shape)\n",
    "print(\"Shape of the X_cv data after Vectorization using BOW \" ,X_cv_summary_bow.shape, Y_cv.shape)\n",
    "print(\"Shape of the X_test_ data after Vectorization using BOW \" ,X_test_summary_bow.shape, Y_test.shape)\n",
    "print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final Data Matrix for Set:1  All the shapes of the data represent the merged features as mentioned in the tittle\n",
      "shape of X_train is :  (30454, 102)\n",
      "shape of X_Cross validation is : (10490, 2746)\n",
      "shape of X_test is  (787, 2746)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "X1_tr = hstack((x_train_clean_categories_ohe,x_train_clean_subcat_ohe,x_train_teacher_ohe,x_train_state_ohe,\\\n",
    "                x_train_grade_ohe,x_train_price_std,x_train_projects_std,x_train_qty_std)).tocsr()\n",
    "X1_cv = hstack((X_cv_clean_categories_ohe,X_cv_clean_subcat_ohe,X_cv_teacher_ohe,X_cv_state_ohe,X_cv_grade_ohe,\\\n",
    "                X_cv_price_std,X_cv_projects_std,X_cv_qty_std,X_cv_titles_bow)).tocsr()\n",
    "X1_te =hstack((X_test_clean_categories_ohe,X_test_clean_subcat_ohe,X_test_teacher_ohe,X_test_state_ohe,\\\n",
    "               X_test_grade_ohe,X_test_titles_bow,X_test_price_std,X_test_projects_std,X_test_qty_std)).tocsr()\n",
    "\n",
    "\n",
    "print(\"The final Data Matrix for Set:1\" , \" All the shapes of the data represent the merged features as mentioned in the tittle\")\n",
    "print(\"shape of X_train is : \",            X1_tr.shape)\n",
    "print(\"shape of X_Cross validation is :\" , X1_cv.shape)\n",
    "print(\"shape of X_test is \",               X1_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Error Plot above shows the best Aplha value as : [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n"
     ]
    }
   ],
   "source": [
    "alpha1= 100\n",
    "print(\"The Error Plot above shows the best Aplha value as :\" , alpha )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(threshould, fpr, tpr):\n",
    "    t = threshould[np.argmax(tpr*(1-fpr))]\n",
    "    print(\"the maximum value of tpr*(1-fpr)\", np.round(max(tpr*(1-fpr)),3), \"for threshold\", np.round(t,3))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_best_t(proba, threshould):\n",
    "    predictions = []\n",
    "    for i in proba:\n",
    "        if i>=threshould:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13e85cd0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(Confusion_metrix_Train_data,annot=True, annot_kws={\"size\": 20},fmt =\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After TFIDF vectorizations of the clean_titles , shape of the data after standazing\n",
      "(30454, 1555) (30454,)\n",
      "(10490, 1555) (10490,)\n",
      "(787, 1555) (787,)\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Applying TF-IDF on Prohect title :\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer9 = TfidfVectorizer(min_df=10)\n",
    "vectorizer9.fit(x_train[\"clean_titles\"].values)\n",
    "\n",
    "x_tain_project_titles_tfidf = vectorizer9.transform(x_train[\"clean_titles\"].values)\n",
    "X_cv_project_titles_tfidf = vectorizer9.transform(X_cv[\"clean_titles\"].values)\n",
    "X_test_project_titles_tfidf = vectorizer9.transform(X_test[\"clean_titles\"].values)\n",
    "\n",
    "\n",
    "print(\"After TFIDF vectorizations of the clean_titles , shape of the data after standazing\")\n",
    "print(x_tain_project_titles_tfidf.shape, y_train.shape)\n",
    "print(X_cv_project_titles_tfidf.shape,Y_cv.shape)\n",
    "print(X_test_project_titles_tfidf.shape, Y_test.shape)\n",
    "print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Error Plot above shows the best Aplha value as : 10\n"
     ]
    }
   ],
   "source": [
    "alpha= 10\n",
    "print(\"The Error Plot above shows the best Aplha value as :\" , alpha )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------------+-----------------------+-----------+\n",
      "| Vectorizer |          Model          | Alpha:Hyper Parameter |  Test AUC |\n",
      "+------------+-------------------------+-----------------------+-----------+\n",
      "|    BOW     | Multinomial Naive Bayes |          0.01         |    0.55   |\n",
      "|   TFIDF    | Multinomial Naive Bayes |          0.01         |    0.68   |\n",
      "+------------+-------------------------+-----------------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "#Conclusion--------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Vectorizer\", \"Model\", \"Alpha:Hyper Parameter\", \" Test AUC\"]\n",
    "x.add_row([\"BOW\", \"Multinomial Naive Bayes\", 0.01, 0.55])\n",
    "x.add_row([\"TFIDF\", \"Multinomial Naive Bayes\", 0.01, 0.68])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
